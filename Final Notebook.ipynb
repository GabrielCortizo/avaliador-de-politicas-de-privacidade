{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8b1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27ecf17",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9660ff",
   "metadata": {},
   "source": [
    "### Pre processing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d2f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import ssl\n",
    "import unicodedata\n",
    "\n",
    "import neattext as nt\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "\n",
    "def to_ascii(text: str) -> str:\n",
    "    return unicodedata.normalize('NFKD', text).encode('ASCII','ignore').decode('ASCII')\n",
    "\n",
    "stopwords = ['a',\n",
    " 'a',\n",
    " 'ao',\n",
    " 'aos',\n",
    " 'aquela',\n",
    " 'aquelas',\n",
    " 'aquele',\n",
    " 'aqueles',\n",
    " 'aquilo',\n",
    " 'as',\n",
    " 'as',\n",
    " 'ate',\n",
    " 'com',\n",
    " 'da',\n",
    " 'das',\n",
    " 'de',\n",
    " 'dela',\n",
    " 'delas',\n",
    " 'dele',\n",
    " 'deles',\n",
    " 'depois',\n",
    " 'do',\n",
    " 'dos',\n",
    " 'e',\n",
    " 'e',\n",
    " 'ela',\n",
    " 'elas',\n",
    " 'ele',\n",
    " 'eles',\n",
    " 'em',\n",
    " 'era',\n",
    " 'eram',\n",
    " 'eramos',\n",
    " 'essa',\n",
    " 'essas',\n",
    " 'esse',\n",
    " 'esses',\n",
    " 'esta',\n",
    " 'esta',\n",
    " 'estamos',\n",
    " 'estao',\n",
    " 'estar',\n",
    " 'estas',\n",
    " 'estava',\n",
    " 'estavam',\n",
    " 'estavamos',\n",
    " 'este',\n",
    " 'esteja',\n",
    " 'estejam',\n",
    " 'estejamos',\n",
    " 'estes',\n",
    " 'esteve',\n",
    " 'estive',\n",
    " 'estivemos',\n",
    " 'estiver',\n",
    " 'estivera',\n",
    " 'estiveram',\n",
    " 'estiveramos',\n",
    " 'estiverem',\n",
    " 'estivermos',\n",
    " 'estivesse',\n",
    " 'estivessem',\n",
    " 'estivessemos',\n",
    " 'estou',\n",
    " 'eu',\n",
    " 'foi',\n",
    " 'fomos',\n",
    " 'for',\n",
    " 'fora',\n",
    " 'foram',\n",
    " 'foramos',\n",
    " 'forem',\n",
    " 'formos',\n",
    " 'fosse',\n",
    " 'fossem',\n",
    " 'fossemos',\n",
    " 'fui',\n",
    " 'ha',\n",
    " 'haja',\n",
    " 'hajam',\n",
    " 'hajamos',\n",
    " 'hao',\n",
    " 'havemos',\n",
    " 'haver',\n",
    " 'hei',\n",
    " 'houve',\n",
    " 'houvemos',\n",
    " 'houver',\n",
    " 'houvera',\n",
    " 'houvera',\n",
    " 'houveram',\n",
    " 'houveramos',\n",
    " 'houverao',\n",
    " 'houverei',\n",
    " 'houverem',\n",
    " 'houveremos',\n",
    " 'houveria',\n",
    " 'houveriam',\n",
    " 'houveriamos',\n",
    " 'houvermos',\n",
    " 'houvesse',\n",
    " 'houvessem',\n",
    " 'houvessemos',\n",
    " 'isso',\n",
    " 'isto',\n",
    " 'ja',\n",
    " 'lhe',\n",
    " 'lhes',\n",
    " 'mais',\n",
    " 'mas',\n",
    " 'me',\n",
    " 'mesmo',\n",
    " 'meu',\n",
    " 'meus',\n",
    " 'minha',\n",
    " 'minhas',\n",
    " 'muito',\n",
    " 'na',\n",
    " 'nao',\n",
    " 'nas',\n",
    " 'nem',\n",
    " 'no',\n",
    " 'nos',\n",
    " 'nos',\n",
    " 'nossa',\n",
    " 'nossas',\n",
    " 'nosso',\n",
    " 'nossos',\n",
    " 'num',\n",
    " 'numa',\n",
    " 'o',\n",
    " 'os',\n",
    " 'ou',\n",
    " 'para',\n",
    " 'pela',\n",
    " 'pelas',\n",
    " 'pelo',\n",
    " 'pelos',\n",
    " 'por',\n",
    " 'que',\n",
    " 'quem',\n",
    " 'sao',\n",
    " 'se',\n",
    " 'seja',\n",
    " 'sejam',\n",
    " 'sejamos',\n",
    " 'sem',\n",
    " 'ser',\n",
    " 'sera',\n",
    " 'serao',\n",
    " 'serei',\n",
    " 'seremos',\n",
    " 'seria',\n",
    " 'seriam',\n",
    " 'seriamos',\n",
    " 'so',\n",
    " 'somos',\n",
    " 'sou',\n",
    " 'tambem',\n",
    " 'te',\n",
    " 'tem',\n",
    " 'tem',\n",
    " 'temos',\n",
    " 'tenha',\n",
    " 'tenham',\n",
    " 'tenhamos',\n",
    " 'tenho',\n",
    " 'tera',\n",
    " 'terao',\n",
    " 'terei',\n",
    " 'teremos',\n",
    " 'teria',\n",
    " 'teriam',\n",
    " 'teriamos',\n",
    " 'teu',\n",
    " 'teus',\n",
    " 'teve',\n",
    " 'tinha',\n",
    " 'tinham',\n",
    " 'tinhamos',\n",
    " 'tive',\n",
    " 'tivemos',\n",
    " 'tiver',\n",
    " 'tivera',\n",
    " 'tiveram',\n",
    " 'tiveramos',\n",
    " 'tiverem',\n",
    " 'tivermos',\n",
    " 'tivesse',\n",
    " 'tivessem',\n",
    " 'tivessemos',\n",
    " 'tu',\n",
    " 'tua',\n",
    " 'tuas',\n",
    " 'um',\n",
    " 'uma',\n",
    " 'voces',\n",
    " 'vos'\n",
    " ]\n",
    "\n",
    "def clean_text(text: str)->List[str]:\n",
    "    # Lower Case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    text = \" \".join([x.lemma_.lower() for x in nlp(text)])\n",
    "\n",
    "    # Remover stopwords\n",
    "    text = \" \".join([ x for x in text.split(' ') if x not in stopwords])\n",
    "\n",
    "    # Remover Acentuaçāo\n",
    "    text = to_ascii(text)\n",
    "\n",
    "    # Remover Pontuaçāo, Números, Urls, emails e caracteres especiais\n",
    "    docx = nt.TextFrame(text=text)\n",
    "    text = docx.remove_puncts().remove_numbers().remove_urls().remove_emails().remove_special_characters().text\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd2422",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d5cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./politica_set.csv')\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b648ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon</td>\n",
       "      <td>notificacao privacidade amazon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon</td>\n",
       "      <td>ultimo atualizacao   junho</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazon</td>\n",
       "      <td>controlador informacao pessoal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon</td>\n",
       "      <td>qual informacao pessoal sobre cliente amazon c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon</td>\n",
       "      <td>qual finalidade amazon tratar seu informacao p...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company                                               text  class\n",
       "0  amazon                     notificacao privacidade amazon      0\n",
       "1  amazon                       ultimo atualizacao   junho        0\n",
       "2  amazon                     controlador informacao pessoal      0\n",
       "3  amazon  qual informacao pessoal sobre cliente amazon c...      1\n",
       "4  amazon  qual finalidade amazon tratar seu informacao p...      2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d350ee1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0     204\n",
       "1      20\n",
       "2      15\n",
       "3      10\n",
       "4      19\n",
       "5      14\n",
       "6       1\n",
       "7       7\n",
       "8      13\n",
       "9       5\n",
       "10      6\n",
       "11     10\n",
       "13     13\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['class'])['class'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f574719",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "138eb509",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "Y = df['class']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a853bc29",
   "metadata": {},
   "source": [
    "## Model Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d6b7f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_tunning(model, params):\n",
    "    gd=GridSearchCV(estimator=model,param_grid=params,verbose=True)\n",
    "    gd.fit(X,Y)\n",
    "    print(gd.best_score_)\n",
    "    print(gd.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd537f",
   "metadata": {},
   "source": [
    "### Tunning SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e4a21714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielcortizo/Desktop/learning/TCC/pdf_retrieval/.venv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8871817383669885\n",
      "SVC(C=1, gamma=0.1, kernel='linear')\n"
     ]
    }
   ],
   "source": [
    "params = {'C':[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "         'gamma':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "         'kernel':['rbf','linear']\n",
    "        }\n",
    "get_best_model_tunning(SVC(), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bba78c",
   "metadata": {},
   "source": [
    "### Tunning Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "62e94733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielcortizo/Desktop/learning/TCC/pdf_retrieval/.venv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7444249341527656\n",
      "MultinomialNB(alpha=0.1)\n"
     ]
    }
   ],
   "source": [
    "params = dict(alpha=np.logspace(0,-9, num=100),fit_prior=[True, False])\n",
    "get_best_model_tunning(MultinomialNB(), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d6e30",
   "metadata": {},
   "source": [
    "### Tunning Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec75cf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielcortizo/Desktop/learning/TCC/pdf_retrieval/.venv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8842405618964004\n",
      "LogisticRegression(C=100)\n"
     ]
    }
   ],
   "source": [
    "params = dict(penalty = ['l2'], C = [0.001,0.01,0.1,1,10,100,1000])\n",
    "get_best_model_tunning(LogisticRegression(), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd64bd92",
   "metadata": {},
   "source": [
    "### Tunning Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac73516",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296000 candidates, totalling 6480000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielcortizo/Desktop/learning/TCC/pdf_retrieval/.venv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = dict(\n",
    "    max_features=list(range(20, 200)), \n",
    "    max_leaf_nodes = range(20, 60, 1),\n",
    "    criterion=['gini', 'entropy'], \n",
    "    max_depth=range(10, 100, 1) \n",
    ")\n",
    "get_best_model_tunning(DecisionTreeClassifier(), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33cdd83",
   "metadata": {},
   "source": [
    "### Tunning Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict( \n",
    "    n_estimators=[200, 500],\n",
    "    max_features=['sqrt','log2'],\n",
    "    criterion=['gini', 'entropy']\n",
    ")\n",
    "get_best_model_tunning(RandomForestClassifier(), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f144d47",
   "metadata": {},
   "source": [
    "### Evaluate Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32432763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "Y = df['class']\n",
    "\n",
    "def get_performance(model):\n",
    "    cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0) \n",
    "    score = cross_val_score(model, X, Y, cv=cv)\n",
    "    print(\"Accuracy: %0.2f (+/-%0.2f)\"%(score.mean(), score.std()*2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f84f6423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67 (+/-0.05)\n",
      "Accuracy: 0.82 (+/-0.08)\n",
      "Accuracy: 0.85 (+/-0.08)\n",
      "Accuracy: 0.86 (+/-0.05)\n",
      "Accuracy: 0.87 (+/-0.03)\n"
     ]
    }
   ],
   "source": [
    "get_performance(MultinomialNB(alpha=0.1))\n",
    "get_performance(RandomForestClassifier())\n",
    "get_performance(DecisionTreeClassifier(max_depth=38, max_features=180, max_leaf_nodes=54))\n",
    "get_performance(LogisticRegression(C=100))\n",
    "get_performance(SVC(C=1, gamma=0.1, kernel='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4489223",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model = SVC(C=1, gamma=0.1, kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c76723",
   "metadata": {},
   "source": [
    "## Saving and importing classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b42d48",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5615983f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving model in hard disk\n",
    "import pickle\n",
    "filename = 'model.sav'\n",
    "pickle.dump(chosen_model, open(filename, 'wb'))\n",
    "\n",
    "#Saving Vectorizer\n",
    "pickle.dump(vectorizer, open('vectorizer.pickle', 'wb')),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112793a",
   "metadata": {},
   "source": [
    "### Importing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "500e5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Model from hard disk\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "#Import Vectorizer\n",
    "vectorizer = pickle.load(open('vectorizer.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c474038",
   "metadata": {},
   "source": [
    "# Testando mais parametros no random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1eeca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "params = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap\n",
    "         }\n",
    "get_best_model_tunning(RandomForestClassifier(), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49eea1",
   "metadata": {},
   "source": [
    "## Evaluating False Negatives and Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f1555434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63,  0,  1,  0,  0,  0,  0,  1,  0,  1,  1],\n",
       "       [ 3,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.3)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_features = vectorizer.fit_transform(train['text'])\n",
    "test_features = vectorizer.transform(test['text'])\n",
    "train_prediction = train['class']\n",
    "test_prediction = test['class']\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "model = chosen_model.fit(train_features, train_prediction)\n",
    "\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "confusion_matrix(test_prediction, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ed26b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
